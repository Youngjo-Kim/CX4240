{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer 2019 CX4240 Homework 1\n",
    "\n",
    "## Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: May 30, Thursday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "In this assignment, we only have writing questions: you are asked to answer them in the markdown cells.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "\n",
    "- Typing with Latex is highly recommended. An image scan copy of handwritten also works. If you hand write, try to be clear as much as possible. No credit may be given to unreadable handwriting.\n",
    "    \n",
    "- If you want to add any picture to your answer, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Eigenvalues and Eigenvectors for Bivariate Gaussian Distribution (25pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let two variables $X_1$ and $X_2$ are bivariately normally distributed with mean vector components $\\mu_1$ and $\\mu_2$ and co-variance matrix $\\Sigma$ shown below:\n",
    "   $$\\Sigma = \n",
    "   \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}$$ \n",
    "\n",
    "- What is the probability distribution function of joint Gaussian $P(X_1, X_2)$? (show it with $\\mu$ and $\\Sigma$) [5pts]\n",
    "\n",
    "    - This is the 2-dimensional nonsingular case because the $rank(\\Sigma) = 2$. Therefore, the joint probability distribution function of $P(X)$ and $P(Y)$ is as below:\n",
    "    \n",
    "    $$P(X_{1})=\\frac{1}{\\sqrt{2\\pi}\\sigma_{11}}e^{-\\frac{\\left(X_{1}-\\mu_{X_{1}} \\right )^2}{2\\sigma_{11}^2}}$$\n",
    "    $$P(X_{2})=\\frac{1}{\\sqrt{2\\pi}\\sigma_{22}}e^{-\\frac{\\left(X_{2}-\\mu_{X_{2}} \\right )^2}{2\\sigma_{22}^2}}$$\n",
    "    \n",
    "    - If two probabilities are independent, we can estimate the joint distribution using the product of both probability distribution functions, but if they are not independent, we cannot estimate the joint distribution using only multiplying. In this case, we have to consider the covariance of both probability.\n",
    "\n",
    "    $$P(X_1, X_2)=\\left(\\frac{1}{2\\pi}\\right)^{\\frac{p}{2}}\\left | \\Sigma \\right |^{-\\frac{1}{2}}\\mathrm{exp}\\left(-\\frac{1}{2} \\left( X-\\mu \\right)^T\\Sigma^{-1}\\left( X-\\mu \\right) \\right)$$\n",
    "\n",
    "    Here, $X=\n",
    "            \\begin{bmatrix}\n",
    "            X_1\\\\\n",
    "            X_2\n",
    "            \\end{bmatrix}$ and $\\mu=\n",
    "            \\begin{bmatrix}\n",
    "            \\mu_1\\\\\n",
    "            \\mu_2\n",
    "            \\end{bmatrix}$\n",
    "            \n",
    "\n",
    "- What are the eigenvalues of co-variance matrix $\\Sigma$? [10pts]\n",
    "    - The definition of eigenvalue for this co-variance matrix is $\\Sigma x = \\lambda x, x \\neq 0$\n",
    "    - To estimate the eigenvalues, the equation becomes $(\\lambda I-\\Sigma)x=0$. We know that $(\\lambda I-\\Sigma)=0$ should meet to get eigenvalues.\n",
    "\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\left |\\lambda I-\n",
    "    \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}\\right |=\n",
    "    \\left |\\begin{bmatrix} \n",
    "    \\lambda-1 & -r \\\\ \n",
    "    -r & \\lambda-1 \n",
    "    \\end{bmatrix}\\right |=0\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    det\\left (\\begin{bmatrix} \n",
    "    \\lambda-1 & -r \\\\ \n",
    "    -r & \\lambda-1 \n",
    "    \\end{bmatrix}\\right ) = (\\lambda-1)^2-r^2=\\lambda^2-2\\lambda+1-r^2=0\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\therefore \\: \\lambda_1 = 1+r, \\: \\: \\lambda_2 = 1-r\n",
    "    $$\n",
    "    \n",
    "    \n",
    "- Given the condition that norm (i.e. the sum of squared values) of each eigenvector is equal to 1, what are the eigenvectors of co-variance matrix $\\Sigma$? ( For example, if an eigenvector is \n",
    "    ${v_1}=\\begin{bmatrix} \n",
    "    x1 \\\\ \n",
    "    x2 \n",
    "    \\end{bmatrix}$, then $x_1^2 + x_2^2 = 1$) [10pts]\n",
    "    \n",
    "    - The egienvectors of symmetric matrix are $v_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ and $v_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}$, but the norm of each eigenvector is 1, so the eigenvectors of this co-variance matrix, $\\Sigma$ are $v_1=\\begin{bmatrix}\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}\\end{bmatrix}$ and $v_2=\\begin{bmatrix}-\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}\\end{bmatrix}$\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Expectation, Co-variance and Independence [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X, Y$ and $Z$ are three different random variables.\n",
    "Let $X$ obeys Bernouli Distribution. The probability disbribution function is\n",
    "    $$p(x)=\\left\\{\n",
    "    \\begin{array}{c l}\t\n",
    "         0.5 & x = 1\\\\\n",
    "         0.5 & x = -1.\n",
    "    \\end{array}\\right.$$\n",
    "    \n",
    "Let $Y$ obeys the standard Normal (Gaussian) distribution, which can be written as $Y \\sim N(0,1)$. $X$ and $Y$ are independent. Meanwhile, let $Z = XY$.\n",
    "\n",
    "- What is the Expectation (mean value) of $X$? [3pts]\n",
    "    - Because the sum of probability is 1, $p(x) = 0,\\:x\\neq\\pm1$\n",
    "    - The expected value of a Bernoulli distributed random variable x with $p(x=1)$ and $p(x=-1)$ is \n",
    "    \n",
    "    $E[X]=p(x=1)\\cdot{1}+p(x=-1)\\cdot{(-1)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.5\\cdot{1}+0.5\\cdot{(-1)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0$\n",
    "    \n",
    "    \n",
    "- Are $Y$ and $Z$ independent? (Just clarify, do not need to prove) [2pts]\n",
    "    - Because $Z=XY$, $Y$ and $Z$ are not independent.\n",
    "    \n",
    "    \n",
    "- Show that $Z$ is also a standard Normal (Gaussian) distribution, which means $Z \\sim N(0,1)$. [10pts]\n",
    "    - Let's consider Q-function, $Q(y)=P(Y\\geq y)$, which is the probability that a standard normal random variable takes a value larger than $y$.\n",
    "    \n",
    "    $P(Z\\geq z)=P(XY \\geq z)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=P(X=1)P(XY\\geq z|X=1)+P(X=-1)P(XY\\geq z|X=-1)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.5P(Y\\geq z|Y=1)+0.5P(-Y\\geq z|Y=-1)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.5P(Y\\geq z)+0.5P(Y\\leq -z)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.5Q(z)+0.5Q(z)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=Q(z)$\n",
    "    \n",
    "    - Therefore, $Z\\sim N(0,1)$\n",
    "        \n",
    "    \n",
    "- Are $Y$ and $Z$ uncorrelated(which means $Cov(Y,Z) = 0$)? (need to prove) [10pts]\n",
    "    - If $Cov(Y,Z)=0$, $Y$ and $Z$ are uncorrelated, otherwise, they are correlated.\n",
    "    \n",
    "    $Cov(Y,Z)=E\\left[ YZ\\right]$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=P(X=1)E\\left[ YZ|X=1\\right]+P(X=-1)E\\left[ YZ|X=-1\\right]$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{1}{2}E\\left[ Y^2\\right]+\\frac{1}{2}E\\left[ -Y^2\\right]$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0$\n",
    "    - Thus, $Y$ and $Z$ are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Maximum Likelihood [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Discrete Example [17pts]\n",
    "Suppose you are playing two unfair coins. The probability of tossing a head is $2 \\theta$ for coin 1, and $\\theta$ for coin 2. You toss each coin for several times, and you get the following results:\n",
    "\n",
    "| Coin No. | Result    |\n",
    "|------|------|\n",
    "|   1  | head |\n",
    "|   2  | head |\n",
    "|   1  | tail |\n",
    "|   2  | tail |\n",
    "|   1  | head |\n",
    "|   2  | tail |\n",
    "\n",
    "- What is the probability of tossing a tail for coin 1 ($p_{t_1}$) and tossing a tail for coin 2 ($p_{t_2}$)[3pts]? \n",
    "    \n",
    "    - Because the probability of tossing a head for coin 1 ($p_{h_1}$) is $2\\theta$, so \n",
    "    \n",
    "    $$p_{t_1} = 1-p_{h_1} = 1-2\\theta$$\n",
    "                \n",
    "    - With the same manner, the probability of tossing a head for coin 2 ($p_{h_2}$) is $\\theta$, so \n",
    "    \n",
    "    $$p_{t_2} = 1-p_{h_2} = 1-\\theta$$\n",
    "    \n",
    "\n",
    "- What is the likelihood of the data given $\\theta$ [7pts]?\n",
    "\n",
    "    - The experimental results show that coin 1 had HTH and coin 2 had HTT. Therefore, the likelihood for coin 1, $L(\\theta|x_1) = p_{h_1}^2 p_{t_1}=(2\\theta)^2(1-2\\theta)$, and the likelihood for coin 2, $L(\\theta|x_2) = p_{h_2} p_{t_2}^2=\\theta(1-\\theta)^2$\n",
    "    \n",
    "    $\\therefore L(\\theta|x) = L(\\theta|x_1)L(\\theta|x_2)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=(2\\theta)^2(1-2\\theta)\\theta(1-\\theta)^2$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=4\\theta^3(1-2\\theta)(1-\\theta)^2$\n",
    "    \n",
    "    \n",
    "- What is maximum likelihood estimation for $\\theta$ [7pts]?\n",
    "\n",
    "    - To get the MLE, we have to differentiate the log of likelihood.\n",
    "    \n",
    "    $\\frac{\\partial {\\text log}L(\\theta|x)}{\\partial \\theta}=\\frac{\\partial}{\\partial \\theta} ({\\text log}4+3{\\text log}\\theta+{\\text log}(1-w\\theta)+2{\\text log}(1-\\theta))$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{3}{\\theta}-\\frac{2}{1-2\\theta}-\\frac{2}{1-\\theta}=0$\n",
    "    \n",
    "    $3(1-2\\theta)(1-\\theta)-2\\theta (1-\\theta)-2\\theta (1-2\\theta)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=3-9\\theta+6\\theta^2-2\\theta+2\\theta^2-2\\theta+4\\theta^2$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=3-13\\theta+12\\theta^2=(3\\theta-1)(4\\theta-3)=0$\n",
    "    \n",
    "    $\\therefore \\hat{\\theta} = \\frac{1}{3} \\:\\:{\\text or} \\:\\:\\hat{\\theta} = \\frac{3}{4}$\n",
    "        \n",
    "    - If $\\hat{\\theta} = \\frac{3}{4}$, the $p_{t_1}=1-2\\frac{3}{4}=-\\frac{1}{2}<0$\n",
    "    \n",
    "    - Because probability should have values between 0 and 1, $\\therefore \\hat{\\theta} = \\frac{1}{3}$\n",
    "    \n",
    "\n",
    "### 3.2 Continues Example [8pts]\n",
    "\n",
    "A uniform distribution in the range of $[0, \\theta]$ is given by\n",
    "\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{\\theta}} & {0 \\leq x \\leq \\theta} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "What is maximum likelihood estimation for $\\theta$?\n",
    "( **hint**: Think of two cases, where $\\theta < max(x_1, x_2, ..., x_n)$ and $\\theta \\ge max(x_1, x_2, ..., x_n).)$\n",
    "\n",
    "<img src=\"https://github.com/Youngjo-Kim/CX4240/blob/master/Homework1_3.2.png?raw=true\" style=\"width: 300px;\">\n",
    "\n",
    "- The likelihood for this uniform distribution is\n",
    "\n",
    "$\n",
    "L(\\theta)=\\prod_{i=1}^{n}P(X_i|\\theta)=\\left\\{\\begin{array}{ll}{\\frac{1}{\\theta ^n}} & {0 \\leq x_i \\leq \\theta} \\:\\:\\:\\: (i=1,...,n) \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$\n",
    "\n",
    "1) $\\theta \\ge max(x_1, x_2, ..., x_n)$,\n",
    "\n",
    "- Because $1/\\theta ^n$ is a decreasing function of $\\theta$, the MLE would be the smallest possible value of $\\theta$ such that $\\theta \\geq x_i$ for $i=1,...,n$.\n",
    "- We can express $\\theta = {\\text max}(x_1,...,x_n)$, and this follows that the maximum likelihood estimateor of $\\theta$ is $\\hat{\\theta}={\\text max}(X_1,...,X_n)$\n",
    "\n",
    "2) $\\theta < max(x_1, x_2, ..., x_n)$,\n",
    "- This condition means there is at least one or more data point which is greater than $\\theta$, then those data points are rejected. After this step, we can do the same procedure with the case of $\\theta \\ge max(x_1, x_2, ..., x_n)$, so finally, the $\\hat{\\theta}={\\text max}(X_1,...,X_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Information Theory [25pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the joint probability distribution of two binary random variables $X$ and $Y$ are given as follows.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline X | Y & {1} & {2} \\\\ \\hline 0 & {\\frac{1}{4}} & {\\frac{1}{4}} \\\\ \\hline 1 & {\\frac{1}{2}} & {0} \\\\ \\hline\\end{array}\n",
    "$$\n",
    "\n",
    "- Show the marginal distribution of $X$. [2pts]\n",
    "    - The definition of margimal distiribution is $p(x)=\\sum_{y}p(x,y)$\n",
    "    - Therefore, $p(0) = {\\frac{1}{4}}+{\\frac{1}{4}} = {\\frac{1}{2}}$ and $p(1) = {\\frac{1}{2}} + 0={\\frac{1}{2}}$\n",
    "    \n",
    "\n",
    "- Find entropy $H(Y)$. [2pts]\n",
    "    - Entropy is estimated with the equation, $H(Y)=-\\sum_{k=1}^{K}P(y=k){\\text {log}}_2P(y=k)$\n",
    "    - In this problem, \n",
    "    \n",
    "    $H(Y)=-\\sum_{k=1}^{2}P(y=k){\\text {log}}_2P(y=k)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=-(P(y=1){\\text {log}}_2P(y=1)+P(y=2){\\text {log}}_2P(y=2))$\n",
    "   \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=-(\\frac{3}{4}{\\text {log}}_2\\frac{3}{4}+\\frac{1}{4}{\\text {log}}_2\\frac{1}{4})$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=-(-0.3113-0.5)=0.8113$\n",
    "    \n",
    "        \n",
    "- Find conditional entropy $H(X|Y)$ and $H(Y|X)$. [3pts]\n",
    "\n",
    "    $H(X|Y) = \\Sigma_{x\\in X, y\\in Y}p(X,Y){\\text log}_2\\frac{p(Y)}{p(X,Y)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=p(X=0,Y=1){\\text log}_2\\frac{p(Y=1)}{p(X=0,Y=1)}+\n",
    "    p(X=0,Y=2){\\text log}_2\\frac{p(Y=2)}{p(X=0,Y=2)}+$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:p(X=1,Y=1){\\text log}_2\\frac{p(Y=1)}{p(X=1,Y=1)}+\n",
    "    p(X=1,Y=2){\\text log}_2\\frac{p(Y=2)}{p(X=1,Y=2)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{1}{4}{\\text log}_2\\frac{3/4}{1/4}+\n",
    "    \\frac{1}{4}{\\text log}_2\\frac{1/4}{1/4}+\n",
    "    \\frac{1}{2}{\\text log}_2\\frac{3/4}{1/2}+0$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:= \\frac{1}{4}{\\text log}_2 3+\\frac{1}{2}{\\text log}_2 \\frac{3}{2}=0.6887$\n",
    "    \n",
    "    $H(Y|X) = \\Sigma_{x\\in X, y\\in Y}p(X,Y){\\text log}_2\\frac{p(X)}{p(X,Y)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=p(X=0,Y=1){\\text log}_2\\frac{p(X=0)}{p(X=0,Y=1)}+\n",
    "    p(X=0,Y=2){\\text log}_2\\frac{p(X=0)}{p(X=0,Y=2)}+$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:p(X=1,Y=1){\\text log}_2\\frac{p(X=1)}{p(X=1,Y=1)}+\n",
    "    p(X=1,Y=2){\\text log}_2\\frac{p(X=1)}{p(X=1,Y=2)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{1}{4}{\\text log}_2\\frac{1/2}{1/4}+\n",
    "    \\frac{1}{4}{\\text log}_2\\frac{1/2}{1/4}+\n",
    "    \\frac{1}{2}{\\text log}_2\\frac{1/2}{1/2}+0$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:= \\frac{1}{4}{\\text log}_2 2+\\frac{1}{4}{\\text log}_2 2$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{1}{2}$\n",
    "    \n",
    "    \n",
    "    \n",
    "- Find mutual information $I(X;Y)$. [3pts]\n",
    "    \n",
    "    $I(X;Y)=H(Y)-H(Y|X)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.8113-0.5 = 0.3113$\n",
    "      \n",
    "    \n",
    "- Find joint entropy $H(X, Y)$. [3pts]\n",
    "\n",
    "    $H(X,Y)= H(\\frac{1}{4},\\frac{1}{4},\\frac{1}{2},0)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=p(X=0,Y=1){\\text log}_2\\frac{1}{p(X=0,Y=1)}+\n",
    "    p(X=0,Y=2){\\text log}_2\\frac{1}{p(X=0,Y=2)}+$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:p(X=1,Y=1){\\text log}_2\\frac{1}{p(X=1,Y=1)}+\n",
    "    p(X=1,Y=2){\\text log}_2\\frac{1}{p(X=1,Y=2)}$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=\\frac{1}{4}{\\text log}_2 4+\n",
    "    \\frac{1}{4}{\\text log}_2 4+\\frac{1}{2}{\\text log}_2 2+0$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=0.5+0.5+0.5 = 1.5$\n",
    "    \n",
    "\n",
    "**Note:** The following three proofs are not related to the example in the above questions. You need to prove each for any general case.\n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X|Y) = H(X)$. [4pts]\n",
    "\n",
    "    - From the definition of the mutual information, $I(X;Y)=H(X)-H(X|Y)$\n",
    "    - If $I(X;Y)=0$, then $H(X)=H(X|Y)$. Thus, if I show $I(X;Y)=0$ when $X$ and $Y$ are independent, I can show $H(X|Y) = H(X)$.\n",
    "    - If $X$ and $Y$ are independent, then $p_{(X,Y)}(x,y)=p_X(x)\\cdot p_Y(y)$\n",
    "    - $\\therefore I(X;Y)=\\sum_{x\\in X,y\\in Y}p(x,y){\\text log}_2\\left ( \\frac{p_{(X,Y)}(x,y)}{p_X(x)\\cdot p_Y(y)} \\right )=\\sum_{x\\in X,y\\in Y}p(x,y){\\text log}_21=0$\n",
    "    \n",
    "    \n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X,Y) = H(X) + H(Y)$. [4pts]\n",
    "\n",
    "    - From the definition, $H(X,Y)= H(X)+I(X;Y)+H(Y|X) = H(Y)+I(X;Y)+H(X|Y)$\n",
    "    - Also, $H(X|Y)=H(X)+I(Y;Y)$ and $H(Y|X)=H(Y)+I(Y;Y)$\n",
    "    - From the above question, we know that $I(X;y)=0$ when $X$ and $Y$ are independent. Therefore, $H(X|Y)=H(X)$ and $H(Y|X)=H(Y)$ if $X$ and $Y$ are independent.\n",
    "    \n",
    "    $\\therefore H(X,Y) = H(X)+I(X;Y)+H(Y|X)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=H(X)+0+H(Y|X)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=H(X)+H(Y)$\n",
    "    \n",
    "    \n",
    "- Show that $I(X; X) = H(X)$. [4pts]\n",
    "\n",
    "    - $H(Y|X)=0$ iff the value of $Y$ is completely determined by the value of $X$.\n",
    "    - Based on this property of conditional entroly, $H(X|X)=0$\n",
    "    \n",
    "    $\\therefore I(X;X) = H(X)-H(X|X)$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=H(X)-0$\n",
    "    \n",
    "    $\\:\\:\\:\\:\\:\\:\\:\\:=H(X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
